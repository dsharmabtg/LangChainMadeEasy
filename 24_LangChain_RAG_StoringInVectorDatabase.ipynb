{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbe67fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "632c8dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_contents(filename):\n",
    "    \"\"\" Given a filename,\n",
    "        return the contents of that file\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(filename, 'r') as f:\n",
    "            # It's assumed our file contains a single line,\n",
    "            # with our API key\n",
    "            return f.read().strip()\n",
    "    except FileNotFoundError:\n",
    "        print(\"'%s' file not found\" % filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e02e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"..\\\\GoogleAPIKey.txt\"\n",
    "os.environ['GOOGLE_API_KEY'] = get_file_contents(filename)\n",
    "filename_grokKey = \"..\\\\GroqAPIKey.txt\"\n",
    "os.environ['GROQ_API_KEY'] = get_file_contents(filename_grokKey)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cfd78d9",
   "metadata": {},
   "source": [
    "## Vector Storage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b58750",
   "metadata": {},
   "source": [
    "### Chroma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b12abc",
   "metadata": {},
   "source": [
    "##### pip install langchain-chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc938255",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_chroma import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30a74fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_model = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7030c446",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the documents and split it into chunks, embed each chunk  and load it into vector store\n",
    "raw_documents = TextLoader(\"..\\\\RAGFiles\\\\LangchainRetrieval.txt\").load()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000,chunk_overlap=0)\n",
    "documents = text_splitter.split_documents(raw_documents)\n",
    "db =  Chroma.from_documents(documents,embeddings_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e42405d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text embedding models\n",
      "Another key part of retrieval is creating embeddings for documents. Embeddings capture the semantic meaning of the text, allowing you to quickly and efficiently find other pieces of a text that are similar. LangChain provides integrations with over 25 different embedding providers and methods, from open-source to proprietary API, allowing you to choose the one best suited for your needs. LangChain provides a standard interface, allowing you to easily swap between models.\n",
      "\n",
      "Vector stores\n",
      "With the rise of embeddings, there has emerged a need for databases to support efficient storage and searching of these embeddings. LangChain provides integrations with over 50 different vectorstores, from open-source local ones to cloud-hosted proprietary ones, allowing you to choose the one best suited for your needs. LangChain exposes a standard interface, allowing you to easily swap between vector stores.\n"
     ]
    }
   ],
   "source": [
    "query=\"What is text embedding and how does langchain help in doing it\"\n",
    "docs = db.similarity_search(query)\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d53abd81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text embedding models\n",
      "Another key part of retrieval is creating embeddings for documents. Embeddings capture the semantic meaning of the text, allowing you to quickly and efficiently find other pieces of a text that are similar. LangChain provides integrations with over 25 different embedding providers and methods, from open-source to proprietary API, allowing you to choose the one best suited for your needs. LangChain provides a standard interface, allowing you to easily swap between models.\n",
      "\n",
      "Vector stores\n",
      "With the rise of embeddings, there has emerged a need for databases to support efficient storage and searching of these embeddings. LangChain provides integrations with over 50 different vectorstores, from open-source local ones to cloud-hosted proprietary ones, allowing you to choose the one best suited for your needs. LangChain exposes a standard interface, allowing you to easily swap between vector stores.\n"
     ]
    }
   ],
   "source": [
    "embedding_vector = embeddings_model.embed_query(query)\n",
    "docs = db.similarity_search_by_vector(embedding_vector)\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c140f09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "print(len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ed18ee35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parent Document Retriever: This allows you to create multiple embeddings per parent document, allowing you to look up smaller chunks but return larger context.\n",
      "Self Query Retriever: User questions often contain a reference to something that isn't just semantic but rather expresses some logic that can best be represented as a metadata filter. Self-query allows you to parse out the semantic part of a query from other metadata filters present in the query.\n",
      "Ensemble Retriever: Sometimes you may want to retrieve documents from multiple different sources, or using multiple different algorithms. The ensemble retriever allows you to easily do this.\n",
      "And more!\n",
      "Indexing\n",
      "The LangChain Indexing API syncs your data from any source into a vector store, helping you:\n",
      "\n",
      "Avoid writing duplicated content into the vector store\n",
      "Avoid re-writing unchanged content\n",
      "Avoid re-computing embeddings over unchanged content\n",
      "All of which should save you time and money, as well as improve your vector search results.\n"
     ]
    }
   ],
   "source": [
    "print(docs[1].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "80f532ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 760, which is longer than the specified 500\n"
     ]
    }
   ],
   "source": [
    "#Load the documents and split it into chunks, embed each chunk  and load it into vector store\n",
    "raw_documents = TextLoader(\"..\\\\RAGFiles\\\\LangchainRetrieval.txt\").load()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=500,chunk_overlap=20)\n",
    "documents = text_splitter.split_documents(raw_documents)\n",
    "db =  Chroma.from_documents(documents,embeddings_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "85523482",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text embedding models\n",
      "Another key part of retrieval is creating embeddings for documents. Embeddings capture the semantic meaning of the text, allowing you to quickly and efficiently find other pieces of a text that are similar. LangChain provides integrations with over 25 different embedding providers and methods, from open-source to proprietary API, allowing you to choose the one best suited for your needs. LangChain provides a standard interface, allowing you to easily swap between models.\n"
     ]
    }
   ],
   "source": [
    "query=\"What is text embedding and how does langchain help in doing it\"\n",
    "docs = db.similarity_search(query)\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "adf8277e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "print(len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "088c1181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text embedding models\n",
      "Another key part of retrieval is creating embeddings for documents. Embeddings capture the semantic meaning of the text, allowing you to quickly and efficiently find other pieces of a text that are similar. LangChain provides integrations with over 25 different embedding providers and methods, from open-source to proprietary API, allowing you to choose the one best suited for your needs. LangChain provides a standard interface, allowing you to easily swap between models.\n",
      "\n",
      "Vector stores\n",
      "With the rise of embeddings, there has emerged a need for databases to support efficient storage and searching of these embeddings. LangChain provides integrations with over 50 different vectorstores, from open-source local ones to cloud-hosted proprietary ones, allowing you to choose the one best suited for your needs. LangChain exposes a standard interface, allowing you to easily swap between vector stores.\n"
     ]
    }
   ],
   "source": [
    "print(docs[1].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7a5afb72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parent Document Retriever: This allows you to create multiple embeddings per parent document, allowing you to look up smaller chunks but return larger context.\n",
      "Self Query Retriever: User questions often contain a reference to something that isn't just semantic but rather expresses some logic that can best be represented as a metadata filter. Self-query allows you to parse out the semantic part of a query from other metadata filters present in the query.\n",
      "Ensemble Retriever: Sometimes you may want to retrieve documents from multiple different sources, or using multiple different algorithms. The ensemble retriever allows you to easily do this.\n",
      "And more!\n",
      "Indexing\n",
      "The LangChain Indexing API syncs your data from any source into a vector store, helping you:\n"
     ]
    }
   ],
   "source": [
    "print(docs[2].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7a0c8a",
   "metadata": {},
   "source": [
    "#### FAISS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfbd8264",
   "metadata": {},
   "source": [
    "##### pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "27abb7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e472e59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the documents and split it into chunks, embed each chunk  and load it into vector store\n",
    "raw_documents = TextLoader(\"..\\\\RAGFiles\\\\LangchainRetrieval.txt\").load()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000,chunk_overlap=0)\n",
    "documents = text_splitter.split_documents(raw_documents)\n",
    "db =  FAISS.from_documents(documents,embeddings_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
